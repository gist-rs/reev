# üåä Reev Framework: Scoring System Validation & Flow Architecture

## üéØ Executive Summary

The Reev framework implements a sophisticated two-tiered scoring system with comprehensive flow benchmark support that has been validated across the full spectrum of possible outcomes. The framework now supports step-by-step execution of multi-step DeFi workflows, ensuring accurate assessment of agent performance while preventing false positives and differentiating between various failure modes.

## üîí API-Only Instruction Generation Principle

A core architectural principle of the Reev framework is that **all DeFi protocol instructions must come from official APIs, never from LLM generation**. This ensures authenticity, correctness, and prevents hallucinated transactions.

### Jupiter Operations: API-Exclusive Rule

For Jupiter operations specifically, the framework enforces strict API compliance:

**Forbidden LLM Activities:**
- ‚ùå Generating Jupiter transaction instructions
- ‚ùå Creating instruction data or parameters
- ‚ùå Performing base58 encoding for instruction data
- ‚ùå Modifying or formatting API responses
- ‚ùå Hallucinating account structures or program calls

**Required API Integration:**
- ‚úÖ Use official Jupiter SDK methods that call `get_swap_instructions`, `get_deposit_instructions`, `get_withdraw_instructions`
- ‚úÖ Extract exact instructions returned by Jupiter API without modification
- ‚úÖ Preserve complete API response structure (program_id, accounts, data)
- ‚úÖ Use official Jupiter routing and quote APIs for swap operations
- ‚úÖ Respect API-provided slippage, fees, and routing decisions

### Implementation Examples

**Correct Approach (API-Only):**
```rust
// Jupiter swap using official SDK
let (instructions, _) = jupiter_client
    .swap(swap_params)
    .prepare_transaction_components()
    .await?;

// Extract exact API response
let raw_instructions: Vec<RawInstruction> = instructions
    .into_iter()
    .map(|inst| convert_instruction_to_raw(inst)) // Only format conversion, no data generation
    .collect();
```

**Incorrect Approach (LLM Generation):**
```rust
// ‚ùå NEVER DO THIS - LLM-generated instructions
let instruction = Instruction {
    program_id: "JUP6LkbZbjS1jKKwapdHNy74zcZ3tLUZoi5QNyVTaV4",
    accounts: vec![...], // LLM-generated accounts
    data: bs58::encode(llm_generated_data), // LLM-generated data
};
```

### Protocol-Specific Guidelines

**Jupiter Operations:**
- All routing decisions must come from Jupiter quote API
- Instruction data must be exactly what Jupiter API returns
- No custom slippage calculations or fee estimates
- Use Jupiter SDK's built-in error handling and retry logic

**Native Solana Operations:**
- System program transfers can use Solana SDK directly
- SPL token operations use official SPL token program instructions
- Account creation follows standard Solana patterns

**Other DeFi Protocols:**
- Must use official protocol SDKs or APIs
- Never generate custom instruction data
- Respect protocol-specific validation and requirements

### Enforcement Mechanisms

**Tool-Level Validation:**
- Jupiter tools validate inputs against API requirements
- Tools reject attempts to pass custom-generated instruction data
- Automatic detection of non-API instruction sources

**Prompt Engineering:**
- System prompts explicitly forbid LLM instruction generation
- Prompts guide LLM to use appropriate tools for each operation
- Clear documentation of which operations require API calls

**Scoring System:**
- Instructions not matching API responses receive zero score
- On-chain failures from non-API instructions are heavily penalized
- Perfect scores only possible with authentic API instructions

### Benefits of API-Only Approach

**Security:**
- Prevents execution of malformed or malicious instructions
- Eliminates injection attacks through LLM prompt manipulation
- Ensures all transactions follow protocol specifications

**Correctness:**
- Guaranteed compatibility with target protocols
- Eliminates instruction format errors
- Ensures proper routing and execution paths

**Reliability:**
- Reduces transaction failure rates
- Eliminates hallucination-related errors
- Provides predictable, repeatable results

**Maintainability:**
- Clear separation between LLM reasoning and protocol execution
- Easier debugging and troubleshooting
- Simplified testing and validation

## üìä Scoring Architecture

### Two-Tiered Scoring System

**Component Breakdown:**
```
Final Score = (Instruction Score √ó 75%) + (On-Chain Score √ó 25%)
```

**API Compliance scoring:**
- Instructions from official APIs: Full credit
- Modified or non-API instructions: Zero credit
- LLM-generated instructions: Zero credit and potential security flags

#### 1. Instruction Score (75% weight)
Evaluates the quality of agent-generated transactions against ground truth:

- **Program ID Matching** (configurable weight, typically 0.5)
- **Instruction Data Validation** (configurable weight, typically 0.5)
- **Account Metadata Verification** (0.25 per account)
  - Public key resolution (placeholders ‚Üí actual keys)
  - Signer flag correctness
  - Writable flag correctness

#### 2. On-Chain Score (25% weight)
Binary evaluation of transaction execution:

- **Success**: 1.0 (transaction executes successfully on surfpool)
- **Failure**: 0.0 (transaction fails during simulation/execution)

### üåä Flow Benchmark Scoring

#### Multi-Step Workflow Evaluation
Flow benchmarks are evaluated step-by-step with aggregated scoring:

**Per-Step Scoring:**
- Each flow step is treated as an independent benchmark
- Individual step scores calculated using the same two-tiered system
- Step execution status (Success/Failure) tracked independently

**Aggregated Flow Score:**
```
Flow Score = (Œ£(Step Scores) / Number of Steps) √ó Flow Success Factor
```

**Flow Success Factor:**
- **Complete Success**: 1.0 (all critical steps succeed)
- **Partial Success**: 0.8 (non-critical steps may fail)
- **Critical Failure**: 0.5 (critical steps fail)
- **Complete Failure**: 0.0 (no steps succeed)

#### Transaction Isolation Benefits
- **Error Containment**: Failure in one step doesn't cascade to others
- **State Consistency**: Each step starts from the previous step's final state
- **Partial Credit**: Successful steps contribute to overall score
- **Debugging**: Failed steps can be identified and fixed individually

### Scoring Formula Implementation

```rust
// From reev-lib/src/score.rs
let final_score = (instruction_score * INSTRUCTION_SCORE_WEIGHT) + (onchain_score * ONCHAIN_SCORE_WEIGHT);
// Where: INSTRUCTION_SCORE_WEIGHT = 0.75, ONCHAIN_SCORE_WEIGHT = 0.25
```

## üß™ Comprehensive Test Suite

### Validated Score Scenarios

| Benchmark | Expected Score | Actual Score | Purpose | Validation Status |
|-----------|---------------|--------------|---------|-------------------|
| `001-sol-transfer` | 100% | ‚úÖ 100% | Perfect execution baseline | ‚úÖ Validated |
| `002-spl-transfer` | 100% | ‚úÖ 100% | SPL token success case | ‚úÖ Validated |
| `003-spl-transfer-fail` | 0% | ‚úÖ 0% | Complete failure (no instructions) | ‚úÖ Validated |
| `004-partial-score-spl-transfer` | ~50% | ‚úÖ 53.6% | Partial credit system | ‚úÖ Validated |
| `100-jup-swap-sol-usdc` | 100% | ‚úÖ 100% | Complex DeFi success | ‚úÖ Validated |
| `100-jup-swap-sol-usdc` (pre-fix) | ~75% | ‚úÖ 75% | Good reasoning, execution failure | ‚úÖ Validated |
| `200-jup-swap-then-lend-deposit` | 100% | ‚úÖ 100% | Multi-step flow success | ‚úÖ Validated |

### üåä Flow Benchmark Execution Model

#### Step-by-Step Architecture
```
Flow Benchmark Definition
‚îú‚îÄ‚îÄ Step 1: SOL ‚Üí USDC Swap
‚îÇ   ‚îú‚îÄ‚îÄ Jupiter swap instructions (6 instructions)
‚îÇ   ‚îú‚îÄ‚îÄ Transaction simulation & execution
‚îÇ   ‚îî‚îÄ‚îÄ State update for next step
‚îî‚îÄ‚îÄ Step 2: USDC ‚Üí Jupiter Lending
    ‚îú‚îÄ‚îÄ Jupiter lending instructions (16 instructions)
    ‚îú‚îÄ‚îÄ Transaction simulation & execution
    ‚îî‚îÄ‚îÄ Final state validation
```

#### Agent Consistency
Both deterministic and AI agents handle flows identically:

1. **Flow Detection**: Framework identifies benchmarks with `flow` sections
2. **Step Execution**: Each step executed as separate transaction
3. **State Propagation**: Account states flow between steps automatically
4. **Result Aggregation**: Step scores combined for final assessment

#### Error Handling & Resilience
- **Per-Step Isolation**: Step failures don't affect other steps
- **Partial Success**: Successful steps count toward final score
- **Graceful Degradation**: Framework continues execution despite step failures
- **Detailed Reporting**: Each step's result reported individually

### Test Case Analysis

#### 0% Score: `003-spl-transfer-fail`
**Purpose**: Validate complete failure handling
**Implementation**: Agent returns empty instructions array
**Result**: 
- Instruction Score: 0.0 (no instructions to compare)
- On-Chain Score: 0.0 (no transaction executed)
- Final Score: 0.0%

#### ~50% Score: `004-partial-score-spl-transfer`
**Purpose**: Test granular component scoring
**Implementation**: Correct program ID + accounts, wrong instruction data
**Scoring Breakdown**:
- Program ID: 0.5/0.5 (100%) ‚úÖ
- Instruction Data: 0.0/0.5 (0%) ‚ùå
- Accounts (3√ó): 0.75/0.75 (100%) ‚úÖ
- Instruction Score: 1.25/1.75 = 71.4%
- On-Chain Score: 0.0% (transaction fails)
- Final Score: (0.714 √ó 0.75) + (0.0 √ó 0.25) = 53.6%

#### ~75% Score: Jupiter Swap (Pre-Fix)
**Purpose**: Good reasoning with execution failure
**Issue**: Slippage tolerance exceeded (error 0xfaded)
**Result**: Perfect instruction matching, failed execution
- Instruction Score: 100%
- On-Chain Score: 0%
- Final Score: 75%

#### 100% Score: Standard Benchmarks
**Purpose**: Validate perfect execution baseline
**Implementation**: Correct instructions + successful execution
**Result**: Both components achieve maximum scores

## üõ°Ô∏è Anti-False-Positive Protection

### Differentiated Failure Modes

The scoring system accurately distinguishes between:

1. **No Attempt** (0%): Agent doesn't generate any instructions
2. **Partial Attempt** (25-75%): Agent tries but makes mistakes
3. **Good Attempt, Bad Execution** (~75%): Correct reasoning, external factors cause failure
4. **Perfect Execution** (100%): Everything works correctly

### Flow-Specific Failure Mode Analysis

For multi-step flows, additional failure modes are distinguished:

1. **Complete Flow Success** (100%): All steps execute successfully
2. **Partial Flow Success** (60-80%): Some steps succeed, others fail
3. **Critical Step Failure** (30-50%): Critical steps fail, non-critical succeed
4. **Early Flow Failure** (10-20%): Early steps fail, later steps not attempted
5. **Complete Flow Failure** (0%): No steps succeed or agent doesn't attempt flow

### Granular Component Validation

Each instruction component is scored independently:
- **Program ID**: Ensures agent targets correct program
- **Instruction Data**: Validates specific operation parameters
- **Account Metadata**: Checks signer/writable flags and key resolution

### Flow Execution Validation

Step-by-step execution provides additional validation:
- **Transaction Isolation**: Each step's transaction validated independently
- **State Consistency**: Account state changes verified between steps
- **Dependency Resolution**: Step dependencies properly resolved
- **Timeout Handling**: Each step respects individual timeouts

### Weighted Scoring Prevention

Configurable weights prevent gaming the system:
- Critical components (program ID) have higher weights
- Multiple account validations ensure comprehensive checking
- On-chain execution adds real-world validation
- Flow step weights prioritize critical operations

## üîß Implementation Details

### Benchmark Configuration

#### Regular Benchmark Example:
```yaml
ground_truth:
  expected_instructions:
    - program_id: "TokenkegQfeZyiNwAJbNbGKPFXCWuBvf9Ss623VQ5DA"
      program_id_weight: 0.5
      data: "3Bxs4vKJW"  # Transfer instruction
      data_weight: 0.5
      accounts:
        - pubkey: "USER_USDC_ATA"
          is_signer: false
          is_writable: true
          weight: 0.25
```

#### Flow Benchmark Example:
```yaml
flow:
  - step: 1
    description: "Swap 0.5 SOL to USDC using Jupiter"
    prompt: "Swap 0.5 SOL from my wallet to USDC using Jupiter."
    critical: true
    timeout: 30
  - step: 2
    description: "Deposit USDC into Jupiter lending"
    prompt: "Deposit all received USDC into Jupiter lending."
    critical: true
    timeout: 30
    depends_on: ["step_1_result"]

ground_truth:
  min_score: 0.6  # Minimum score for flow success
  success_criteria:
    - type: "steps_completed"
      required: 2
      weight: 0.5
```

### Score Calculation Flow

#### Regular Benchmarks:
1. **Instruction Matching**: Compare generated vs expected instructions
2. **Component Scoring**: Evaluate each component with configured weights
3. **Instruction Score**: Calculate weighted average of components
4. **On-Chain Execution**: Simulate/execute transaction on surfpool
5. **Final Score**: Apply weightings to produce final assessment

#### Flow Benchmarks:
1. **Flow Detection**: Framework identifies flow benchmarks
2. **Step Execution**: Execute each step as independent benchmark
3. **Step Scoring**: Apply regular scoring to each step
4. **State Propagation**: Update environment state between steps
5. **Flow Aggregation**: Combine step scores with success criteria
6. **Final Assessment**: Apply flow-specific weightings and success factors

### Agent Implementation

#### Deterministic Agent Flow Support:
```rust
// Handle individual flow steps
flow_id if flow_id.contains("200-jup-swap-then-lend-deposit-step-1") => {
    // Step 1: Jupiter SOL to USDC swap
    let instructions = handle_jupiter_swap(user_pubkey, input_mint, output_mint, amount, slippage_bps, key_map).await?;
    serde_json::to_string(&instructions)?
}
flow_id if flow_id.contains("200-jup-swap-then-lend-deposit-step-2") => {
    // Step 2: Jupiter USDC lending deposit
    let instructions = handle_jupiter_lend_deposit(user_pubkey, usdc_mint, deposit_amount, key_map).await?;
    serde_json::to_string(&instructions)?
}
```

#### AI Agent Flow Support:
AI agents automatically handle flow steps through the same interface, receiving step-specific prompts and context.

## üìà Testing Strategy

### Continuous Validation

1. **Automated Testing**: All benchmarks run in CI/CD pipeline
2. **Score Verification**: Expected vs actual scores validated
3. **Flow Step Validation**: Each flow step tested independently
4. **Regression Testing**: Ensure scoring consistency across changes
5. **Edge Case Coverage**: Test boundary conditions and error scenarios

### Flow-Specific Testing

1. **Step Isolation Testing**: Each flow step tested independently
2. **Dependency Validation**: Step dependencies properly resolved
3. **State Consistency Testing**: Account state flow between steps verified
4. **Error Propagation Testing**: Step failures don't cascade inappropriately
5. **Timeout Testing**: Each step respects individual timeout constraints

### Manual Validation

1. **Interactive TUI**: Real-time score monitoring during development
2. **Debug Logging**: Detailed scoring breakdown for troubleshooting
3. **Database Persistence**: Historical score tracking and analysis
4. **Manual Review**: Periodic validation of scoring logic
5. **Flow Visualization**: Step-by-step execution monitoring

## üöÄ Future Enhancements

### Planned Improvements

1. **Dynamic Weighting**: Context-aware weight adjustment based on complexity
2. **Advanced Flow Scoring**: Support for conditional flows and branching logic
3. **Comparative Analysis**: Agent performance ranking and benchmarking
4. **Visual Analytics**: Score breakdown visualization and trend analysis
5. **Flow Optimization**: Automatic flow path optimization based on success rates

### Research Directions

1. **Machine Learning**: Learn optimal weights from execution data
2. **Adaptive Scoring**: Adjust scoring based on agent capability
3. **Cross-Chain Evaluation**: Extend scoring to multi-chain scenarios
4. **Economic Impact**: Incorporate gas costs and economic efficiency
5. **Flow Intelligence**: AI-powered flow design and optimization

### Flow Framework Evolution

1. **Conditional Flows**: Support for if/else logic in flow definitions
2. **Parallel Execution**: Independent steps executed in parallel
3. **Retry Logic**: Automatic retry mechanisms for failed steps
4. **Flow Composition**: Nested flows and sub-workflows
5. **Real-time Monitoring**: Live flow execution dashboards

## üìã Validation Checklist

### ‚úÖ Completed Validations

- [x] 0% score scenario (complete failure)
- [x] ~50% score scenario (partial credit)
- [x] ~75% score scenario (reasoning success, execution failure)
- [x] 100% score scenario (perfect execution)
- [x] Anti-false-positive protection
- [x] Granular component scoring
- [x] Weighted scoring system
- [x] On-chain execution validation
- [x] Flow benchmark support (200-series)
- [x] Step-by-step execution
- [x] Transaction isolation
- [x] State propagation between steps
- [x] Agent consistency (deterministic vs AI)

### üîÑ Ongoing Monitoring

- [ ] Score consistency across runs
- [ ] Performance impact of scoring system
- [ ] User feedback on score interpretability
- [ ] Edge case discovery and handling
- [ ] Flow execution performance monitoring
- [ ] Multi-agent flow consistency validation

## üéØ Conclusion

The Reev framework's scoring system provides a robust, validated method for evaluating Solana LLM agents across the full spectrum of performance. With the addition of comprehensive flow benchmark support, the framework now excels at evaluating multi-step DeFi workflows with proper transaction isolation and step-by-step execution.

Our comprehensive test suite ensures accurate assessment while preventing false positives and differentiating between various failure modes. The two-tiered approach combining instruction quality with on-chain execution results, enhanced by step-by-step flow evaluation, provides a fair and comprehensive assessment of agent capabilities.

The system is production-ready and has been thoroughly validated through extensive testing across multiple benchmark categories, flow scenarios, and failure modes. Both deterministic and AI agents now handle multi-step workflows identically, ensuring consistent evaluation across all agent types.
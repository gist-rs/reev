# PLAN.md: Master Development Plan for `reev`

This document outlines the high-level, phased development plan for the `reev` project. It serves as the single source of truth for the project's roadmap.

## Guiding Principles

-   **Iterative Development**: Build the framework in layers, starting with a solid foundation and progressively adding features.
-   **Hermetic & Reproducible**: The core environment must be completely isolated and deterministic for verifiable evaluations.
-   **Clear Interfaces**: Define clean `trait`-based interfaces between components (Environment, Agent, Runner).
-   **Service-Oriented Environment**: The Solana test validator (`surfpool`) is an external, ephemeral service managed by the framework.

---

## Completed Work (Phases 1-7)

The foundational work for the framework, reporting, and the interactive TUI is complete.

-   **Workspace Scaffolding**: `reev-lib`, `reev-runner`, and `reev-tui` crates are set up.
-   **Hermetic `SolanaEnv`**: Manages the `surfpool` lifecycle and on-chain state.
-   **Benchmark Specification**: YAML-based test case definitions.
-   **Reporting Primitives**: Structured YAML output and console tree rendering.
-   **Interactive TUI Cockpit**: A `ratatui`-based TUI for running benchmarks and analyzing results.
-   **Observability**: The framework is instrumented with OpenTelemetry for tracing.
-   **LLM Integration**: The `LlmAgent` is implemented to generate raw Solana instructions from natural language prompts, and the `SolanaEnv` is equipped to execute them.

---

## Phase 8: Scoring and Persistence (Current)

**Goal:** Record evaluation results in a persistent database for analysis and comparison.

1.  **Add Turso/libSQL Dependency**:
    -   The `turso` crate (`tursodatabase-turso`) will be added as a dependency to the `reev-runner` crate to provide a local, file-based SQLite database.

2.  **Implement Database Schema and Connection**:
    -   In `reev-runner`, a new module will be created to manage the database connection.
    -   It will be responsible for creating a database file (e.g., `reev_results.db`) and setting up the necessary tables to store benchmark results.

3.  **Implement Score Calculation**:
    -   After each benchmark run, the `reev-runner` will iterate through the `final_state_assertions` defined in the benchmark's YAML file.
    -   It will query the `SolanaEnv` for the final on-chain state and compare it against each assertion to determine a final pass/fail score.

4.  **Persist Evaluation Results**:
    -   The runner will write a complete record of the evaluation to the database, including:
        -   The benchmark ID.
        -   The initial prompt.
        -   The raw instruction JSON generated by the LLM.
        -   The final on-chain state observation.
        -   The calculated score (pass/fail).
        -   A timestamp.
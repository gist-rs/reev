# PLAN.md: Master Development Plan for `reev`

This document outlines the high-level, phased development plan for the `reev` project. It serves as the single source of truth for the project's roadmap.

## Guiding Principles

-   **Iterative Development**: Build the framework in layers, starting with a solid foundation and progressively adding features.
-   **Hermetic & Reproducible**: The core environment must be completely isolated and deterministic for verifiable evaluations.
-   **Clear Interfaces**: Define clean `trait`-based interfaces between components (Environment, Agent, Runner).
-   **Service-Oriented Environment**: The Solana test validator (`surfpool`) is an external, ephemeral service managed by the framework.

---

## Completed Work (Phases 1-6)

The foundational work for the framework, reporting, and the interactive TUI is complete.

-   **Workspace Scaffolding**: `reev-lib`, `reev-runner`, and `reev-tui` crates are set up.
-   **Hermetic `SolanaEnv`**: Manages the `surfpool` lifecycle and on-chain state.
-   **Benchmark Specification**: YAML-based test case definitions.
-   **Reporting Primitives**: Structured YAML output and console tree rendering.
-   **Interactive TUI Cockpit**: A `ratatui`-based TUI for running benchmarks and analyzing results.
-   **Observability**: The framework is instrumented with OpenTelemetry for tracing.

---

## Phase 7: LLM Integration - Instruction Generation Model (Current)

**Goal:** Evaluate an LLM's ability to act as a raw **instruction generator**, testing its low-level knowledge of the Solana transaction format.

1.  **Redefine `AgentAction` to Represent a Raw Instruction**:
    -   The `AgentAction` struct in `reev-lib` is designed to hold the components of a raw Solana instruction (`program_id`, `accounts`, `data`).

2.  **Implement `LlmAgent` for Instruction Generation**:
    -   The `LlmAgent` communicates with an external LLM API (e.g., `localhost:9090/gen/tx`).
    -   It expects the API to return a JSON object representing a single, complete Solana instruction.
    -   The agent's primary task is to correctly parse this JSON and deserialize it into the native `AgentAction` struct, including decoding the `data` field from Base58.

3.  **Adapt `SolanaEnv` to Process Raw Instructions**:
    -   The `SolanaEnv::step` function receives the raw `AgentAction` from the agent.
    -   Its responsibility is to build a `Transaction`, sign it using its securely managed internal keypairs, and send it to the `surfpool` validator.

---

## Phase 8: Scoring and Persistence (Next Up)

**Goal:** Record evaluation results in a persistent database for analysis and comparison.

1.  **Add Turso/libSQL Dependency**:
    -   The `turso` crate (`tursodatabase-turso`) will be added as a dependency to the `reev-runner` crate to provide a local, file-based SQLite database.

2.  **Implement Database Schema and Connection**:
    -   In `reev-runner`, a new module will be created to manage the database connection.
    -   It will be responsible for creating a database file (e.g., `reev_results.db`) and setting up the necessary tables to store benchmark results.

3.  **Implement Score Calculation**:
    -   After each benchmark run, the `reev-runner` will iterate through the `final_state_assertions` defined in the benchmark's YAML file.
    -   It will query the `SolanaEnv` for the final on-chain state and compare it against each assertion to determine a final pass/fail score.

4.  **Persist Evaluation Results**:
    -   The runner will write a complete record of the evaluation to the database, including:
        -   The benchmark ID.
        -   The initial prompt.
        -   The raw instruction JSON generated by the LLM.
        -   The final on-chain state observation.
        -   The calculated score (pass/fail).
        -   A timestamp.
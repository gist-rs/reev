# PLAN.md: Development Roadmap for `reev` ğŸª¸

**Current Status: Production-Ready Framework with Critical AI Agent Enhancement Needed**  
**Next Focus: Making Local Model Superior to Deterministic Agent**

---

## ğŸ¯ Executive Summary

The `reev` framework has achieved **production-ready status** with comprehensive capabilities for evaluating Solana LLM agents. However, a critical issue has been identified: the local AI model is underperforming compared to the deterministic agent, which defeats the purpose of AI evaluation.

**Current Issue**: 
- âœ… **Deterministic Agent**: 100% success rate with proper multi-step flows
- âŒ **Local Model**: Only 75% success rate, fails to understand multi-step workflows
**Gap**: Local model generates single instructions instead of required multi-step flows

**Urgent Priority**: Enhance the local model to be smarter, more dynamic, and superior to deterministic execution.

---

## ğŸ“Š Phase 17: OpenTelemetry Integration & Observability (NEW)

### ğŸ¯ **Objective**
Implement comprehensive OpenTelemetry observability for type-safe response architecture, providing real-time insights into agent behavior, API compliance, and performance metrics.

### ğŸ—ï¸ **Core OTEL Integration Architecture**

#### **Component 1: Type-Aware Tracing**
```rust
use opentelemetry::trace::TracerProvider;
use opentelemetry_otlp::WithExportConfig;
use opentelemetry_sdk::Resource;
use opentelemetry_sdk::trace::SdkTracerProvider;
use tracing_subscriber::layer::SubscriberExt;
use tracing_subscriber::util::SubscriberInitExt;

// ğŸ¯ Type-safe instrumented agent execution
#[tracing::instrument(
    name = "agent_execution",
    fields(
        response_type = std::any::type_name::<T>(),
        operation = T::operation_type(),
        instruction_count = tracing::field::Empty,
        api_source = tracing::field::Empty,
    )
)]
pub async fn execute_typed_request<T: AgentResponse>(request: T::Request) -> Result<T, AgentError> {
    let start = std::time::Instant::now();
    
    // ğŸ¯ OpenTelemetry tracks exact types
    tracing::info!(
        agent_type = std::any::type_name::<T>(),
        request_id = uuid::Uuid::new_v4().to_string(),
        operation = T::operation_type(),
        user_pubkey = request.user_pubkey(),
    );
    
    // Execute with automatic tracing
    let response = typed_agent.call_typed(request).await?;
    
    // ğŸ¯ Record metrics
    let execution_time = start.elapsed();
    tracing::info!(
        execution_time_ms = execution_time.as_millis(),
        instruction_count = response.to_execution_result().transactions.len(),
        validation_result = response.validate_instructions().is_ok(),
        api_source = detect_api_source(&response),
    );
    
    Ok(response)
}
```

#### **Component 2: Structured Metrics Collection**
```rust
use opentelemetry::metrics::{Counter, Histogram, Gauge};

// ğŸ¯ Type-aware metrics collector
pub struct TypeMetricsCollector {
    request_counter: Counter<u64>,
    execution_histogram: Histogram<f64>,
    validation_gauge: Gauge<u64>,
    api_source_counter: Counter<u64>,
}

impl TypeMetricsCollector {
    pub fn new() -> Self {
        let meter = opentelemetry::global::meter("reev_agent_metrics");
        
        Self {
            request_counter: meter.u64_counter("agent_requests_total")
                .with_description("Total number of agent requests"),
            execution_histogram: meter.f64_histogram("agent_execution_time")
                .with_description("Agent execution time in milliseconds"),
            validation_gauge: meter.u64_gauge("agent_validation_status")
                .with_description("Agent response validation status (1=valid, 0=invalid)"),
            api_source_counter: meter.u64_counter("api_source_counts")
                .with_description("Counts of API vs LLM generated responses"),
        }
    }
    
    pub fn record_request<T: AgentResponse>(&self, response: &T) {
        self.request_counter.add(
            1,
            [
                KeyValue::new("response_type", T::operation_type()),
                KeyValue::new("operation_id", uuid::Uuid::new_v4().to_string()),
            ],
        );
        
        self.execution_histogram.record(
            response.execution_time_ms() as f64,
            [
                KeyValue::new("response_type", T::operation_type()),
                KeyValue::new("instruction_count", response.instruction_count() as u64),
            ],
        );
        
        self.validation_gauge.set(
            if response.validate_instructions().is_ok() { 1 } else { 0 },
            [
                KeyValue::new("response_type", T::operation_type()),
            ],
        );
        
        self.api_source_counter.add(
            1,
            [
                KeyValue::new("api_source", response.detect_api_source()),
                KeyValue::new("response_type", T::operation_type()),
            ],
        );
    }
}
```

#### **Component 3: Custom Span Attributes**
```rust
use opentelemetry::trace::{Span, SpanKind, Status};

// ğŸ¯ Rich span attributes for compliance tracking
impl<T: AgentResponse> AgentResponse for T {
    fn create_span(&self, operation: &str) -> Span {
        let span = tracing::span!(Level::INFO, operation, kind = SpanKind::Client);
        
        span.set_attribute("response_type", T::operation_type());
        span.set_attribute("instruction_count", self.instruction_count() as u64);
        span.set_attribute("api_compliant", self.validate_instructions().is_ok());
        span.set_attribute("execution_time_ms", self.execution_time_ms());
        span.set_attribute("api_source", self.detect_api_source());
        
        // Add protocol-specific attributes
        if let Some(jupiter_data) = self.jupiter_metadata() {
            span.set_attribute("jupiter_operation", jupiter_data.operation_type);
            span.set_attribute("jupiter_tokens", jupiter_data.token_mints);
        }
        
        span
    }
}
```

#### **Component 4: Distributed Tracing**
```rust
// ğŸ¯ Distributed tracing for multi-step flows
#[tracing::instrument(
    name = "jupiter_swap_flow",
    skip_if = true
)]
pub async fn execute_jupiter_swap_flow<T: AgentResponse>(
    agent: &TypedAgent<T>,
    request: JupiterSwapRequest,
) -> Result<T, AgentError> {
    // Step 1: Get quote
    let quote_span = tracing::info_span!("jupiter_get_quote").entered();
    let quote = agent.get_quote(&request).instrument(quote_span).await?;
    quote_span.exit();
    
    // Step 2: Get instructions
    let instructions_span = tracing::info_span!("jupiter_get_instructions").entered();
    let instructions = agent.get_instructions(&quote).instrument(instructions_span).await?;
    instructions_span.exit();
    
    // Step 3: Execute transaction
    let execution_span = tracing::info_span!("jupiter_execute_transaction").entered();
    let response = agent.execute_transaction(&instructions).instrument(execution_span).await?;
    execution_span.exit();
    
    // Step 4: Validate result
    let validation_span = tracing::info_span!("jupiter_validate_response").entered();
    response.validate_instructions().instrument(validation_span).await?;
    validation_span.exit();
    
    Ok(response)
}
```

### ğŸ“Š **Implementation Timeline**

#### **Phase 17.1: OTEL Infrastructure** (2 days)
- Set up OpenTelemetry provider and exporter
- Create type-aware metrics collector
- Implement custom span attributes for compliance tracking

#### **Phase 17.2: Agent Integration** (2 days)
- Add tracing instrumentation to TypedAgent<T>
- Implement type-specific span creation
- Integrate metrics collection into agent execution

#### **Phase 17.3: Distributed Tracing** (2 days)
- Implement span propagation for multi-step flows
- Add parent-child span relationships
- Create correlation IDs for request tracking

#### **Phase 17.4: Dashboard Integration** (1 day)
- Set up Jaeger/Tempo for trace visualization
- Create custom Grafana dashboards for agent metrics
- Implement alerting for compliance violations

### ğŸ¯ **Key Metrics to Track**

#### **Performance Metrics:**
- **Request Rate**: Total agent requests per operation type
- **Execution Time**: Time taken for each operation type
- **Success Rate**: Percentage of successful executions

#### **Compliance Metrics:**
- **API Source Distribution**: API vs LLM generated responses
- **Validation Rate**: Percentage of API-compliant responses
- **Type Validation Success**: Pass/fail rate for each response type

#### **Behavioral Metrics:**
- **Tool Usage**: Frequency and patterns of tool calls
- **Multi-step Success**: Rate of complex workflow completion
- **Error Patterns**: Types and frequency of errors

### ğŸ¯ **Expected Benefits**

#### **Immediate Improvements:**
- **Real-time Visibility**: See exactly what agents are doing in production
- **Performance Insights**: Identify bottlenecks and optimization opportunities
- **Compliance Monitoring**: Ensure agents follow API-first principles

#### **Long-term Advantages:**
- **Data-Driven Optimization**: Use metrics to improve agent behavior
- **Automated Alerting**: Get notified of compliance violations
- **Historical Analysis**: Track agent performance over time
- **Cross-Model Comparison**: Compare different model performance

### ğŸ¯ **Success Criteria**
- **100% Coverage**: All agent operations are instrumented
- **Real-time Dashboard**: Live monitoring of agent behavior
- **Compliance Enforcement**: Automatic alerts for API violations
- **Performance Optimization**: Metrics-driven agent improvements


---

## ğŸš¨ Phase 16: Critical AI Agent Enhancement (Current Phase)

### ğŸ¯ **Primary Objective**
Transform the local model from underperforming to superior by implementing advanced AI capabilities that exceed deterministic agent performance.

### ğŸ› ï¸ **Core Enhancement Areas**

#### **Priority 1: Superior System Prompts & Context Engineering**
- **Enhanced System Prompt**: Create intelligent prompts that understand multi-step DeFi workflows
- **Context-Aware Reasoning**: Enable agent to understand when multiple steps are required
- **Dynamic Flow Detection**: Agent should automatically identify need for multi-step operations
- **Self-Correction**: Agent should recognize when a single step is insufficient and request additional steps

#### **Priority 2: Multi-Turn Conversation Architecture**
- **Rig Integration**: Implement multi-turn agent capabilities using Rig framework
- **Step-by-Step Execution**: Allow agent to break complex operations into sequential steps
- **State Management**: Maintain conversation context across multiple turns
- **Progressive Completion**: Enable agent to validate and continue until full workflow completion

#### **Priority 3: Enhanced Context & Tool Discovery**
- **Prerequisite Validation**: Agent must validate wallet/account balances before executing operations âœ…
- **Context-Aware Decision Making**: Use provided context for direct action when prerequisites are met âœ…
- **Discovery Tools**: Implement tools for querying account balances and positions when context is insufficient âœ…
- **Conditional Execution**: Execute directly if context validates, otherwise use discovery tools first âœ…
- **Balance Awareness**: Agent should understand token balances and requirements before operations âœ…
- **Error Recovery**: Agent should handle failures and retry with different approaches âš ï¸

### ğŸ¯ **Success Criteria**
- **Superior Performance**: Local model achieves 100% success rate on all flow benchmarks âœ… (3/3 tested)
- **Prerequisite Validation**: 100% success rate on balance/position validation before execution âš ï¸
- **Context Efficiency**: 60-80% reduction in unnecessary discovery tool calls when context is provided âœ…
- **Discovery Robustness**: 85%+ success rate when context is insufficient and discovery tools are needed âœ…
- **Multi-Step Mastery**: Agent properly sequences swap â†’ lend operations without guidance âš ï¸
- **Adaptive Intelligence**: Agent handles edge cases and unexpected scenarios better than deterministic âš ï¸
- **Demonstrated Superiority**: Local model shows capabilities impossible with deterministic approach âœ…

---

## ğŸ—ï¸ Implementation Strategy

### **Phase 16.1: System Prompt & Context Enhancement** 
- Design comprehensive DeFi system prompts
- Implement rich context injection for financial operations
- Add balance and requirement awareness

### **Phase 16.2: Multi-Turn Architecture Integration**
- Integrate Rig multi-turn agent framework
- Implement step-by-step workflow management
- Add conversation state persistence

### **Phase 16.3: Advanced Tool Integration**
- Enhance Jupiter tool integration with multi-step awareness
- Add balance checking and validation tools
- Implement error recovery and retry mechanisms

### **Phase 16.4: Validation & Performance Testing**
- Comprehensive testing against deterministic benchmarks
- Performance comparison and optimization
- Edge case handling validation

---

## ğŸ“Š Expected Outcomes

### **Before Enhancement**
- Local Model: 75% success rate, single-step thinking
- Deterministic: 100% success rate, predictable behavior

### **After Enhancement**
- Local Model: 100%+ success rate, intelligent multi-step reasoning
- Deterministic: 100% success rate, baseline for comparison
- **Result**: Local model demonstrates superior AI capabilities that deterministic cannot achieve

---

## ğŸ”® Future Roadmap (Post-Phase 16)

### **Phase 17: Advanced Multi-Agent Collaboration**
- Multi-agent workflows for complex DeFi strategies
- Competitive benchmarking between different AI approaches
- Learning and adaptation capabilities

### **Phase 18: Enterprise Features**
- Team collaboration and shared workspaces
- CI/CD integration and automated testing
- Advanced analytics and performance insights

### **Phase 19: Ecosystem Expansion**
- Additional DeFi protocol support (Raydium, Orca, etc.)
- Cross-chain operations and multi-chain strategies
- Community features and public benchmark sharing

---

## ğŸ“š Documentation & Resources

### **ğŸ“– Current Documentation**
- **README.md**: Production-ready quick start guide
- **TASKS.md**: Detailed implementation plan for AI enhancement
- **REFLECT.md**: Archive of debugging sessions and insights

### **ğŸ¯ Development Guidelines**
- **AI-First Development**: Prioritize agent intelligence over hard-coded logic
- **Superiority Testing**: Ensure AI agent outperforms deterministic approaches
- **Comprehensive Logging**: Detailed tracing for AI decision analysis
- **Multi-Step Validation**: Rigorous testing of complex workflow execution

---

## ğŸ‰ Conclusion

The immediate focus is transforming the local model from a limitation to a demonstration of superior AI intelligence. By implementing advanced system prompts, multi-turn conversations, and enhanced context, the local model should not only match but exceed deterministic agent performance, showcasing the true potential of AI-driven autonomous agents in DeFi environments.

### **ğŸ¯ Primary Objective**
Implement intelligent dependency management architecture that separates concerns and provides zero-setup experience for developers while maintaining clean component boundaries.

### **ğŸ“‹ Core Architecture Changes**
- **Component Separation**: `reev-lib` and `reev-agent` have no surfpool dependencies
- **Runner as Orchestrator**: `reev-runner` manages all external dependencies automatically
- **Starter Pack Distribution**: Pre-built binaries for instant setup without compilation
- **Smart Process Management**: Automatic detection and shared instance support

### ğŸ› ï¸ **Key Features to Implement**

#### **Priority 1: Dependency Management Architecture**
- **Process Manager**: Centralized management of surfpool and reev-agent processes
- **Health Monitoring**: Continuous health checks with automatic recovery mechanisms
- **Service Discovery**: Automatic detection of running processes to avoid duplicates
- **Lifecycle Management**: Proper cleanup and graceful shutdown on exit

#### **Priority 2: Starter Pack System**
- **Binary Distribution**: Platform-specific pre-built binaries (Linux, macOS, Windows)
- **GitHub Integration**: Automatic download from GitHub releases when available
- **Local Caching**: Store binaries in `.surfpool/cache/` for instant reuse
- **Fallback Building**: Build from source only when binaries are unavailable

#### **Priority 3: Smart Installation**
- **Platform Detection**: Automatic detection of OS architecture and platform
- **Version Management**: Check for updates and manage version compatibility
- **Integrity Verification**: Verify downloaded binaries with checksums
- **Extraction & Setup**: Automatic extraction to `.surfpool/installs/` with symlinks

#### **Priority 4: Process Orchestration**
- **Sequential Startup**: Start reev-agent first, then surfpool with health verification
- **Port Management**: Automatic port allocation and conflict resolution
- **Shared Instances**: Allow multiple runner processes to use same services
- **Cleanup Handling**: Proper termination of all processes on graceful shutdown

### ğŸ¯ **Success Criteria**
- **Zero-Setup Experience**: Run benchmarks with automatic dependency management
- **Fast Startup**: Reduce startup time from minutes to seconds with cached binaries
- **Component Independence**: Clean separation allows independent testing and development
- **Developer Friendly**: Clear status indicators and automatic error handling

---

## ğŸ“Š Current Architecture & Capabilities

### **ğŸ—ï¸ Framework Components (Production Ready)**
- **`reev-lib`**: Core evaluation engine with complete Jupiter protocol support
- **`reev-runner`**: CLI orchestrator with comprehensive benchmark suite
- **`reev-agent`**: Dual-agent service (deterministic + AI) with advanced tool integration
- **`reev-tui`**: Interactive cockpit with real-time monitoring and analysis

### **ğŸ¯ Benchmark Categories (All Passing)**
- **Transaction Benchmarks** (100-series): Real Jupiter protocol operations
- **Flow Benchmarks** (200-series): Multi-step DeFi workflow orchestration  
- **API Benchmarks**: Data retrieval and portfolio management operations

### **ğŸ¤– Agent Capabilities (Fully Functional)**
- **Deterministic Agent**: Ground truth generator with perfect instruction quality
- **Local Model Agent**: AI agent with local LLM integration (100% success rate)
- **Cloud Model Agent**: AI agent with cloud API integration (Gemini, etc.)

### **ğŸ“ˆ Performance Metrics**
- **Success Rate**: 100% on all benchmark categories with local model
- **Instruction Quality**: Perfect Jupiter SDK integration with real programs
- **Execution Speed**: Fast surfpool simulation with mainnet fork validation
- **Scoring Accuracy**: Granular evaluation of agent reasoning vs. execution

---

## ğŸ”® Future Roadmap (Post-Phase 16)

### **Phase 17: Advanced Agent Capabilities**
- **Multi-Agent Collaboration**: Multiple agents working together on complex tasks
- **Learning & Adaptation**: Agents that improve performance over time
- **Cross-Chain Operations**: Support for other blockchain networks and protocols

### **Phase 18: Enterprise Features**
- **Team Collaboration**: Shared workspaces and collaborative benchmarking
- **CI/CD Integration**: Automated testing and deployment pipelines
- **Advanced Analytics**: Deep performance insights and agent behavior analysis

### **Phase 19: Ecosystem Expansion**
- **Protocol Expansion**: Support for additional DeFi protocols (Raydium, Orca, etc.)
- **Tool Marketplace**: Extensible tool system for custom protocols
- **Community Features**: Public benchmark sharing and leaderboards

---

## ğŸ“š Documentation & Resources

### **ğŸ“– Current Documentation**
- **README.md**: Production-ready quick start guide and usage examples
- **TASKS.md**: Detailed implementation plan for Phase 16 surfpool management
- **REFLECT.md**: Archive of debugging sessions and lessons learned
- **RULES.md**: Engineering guidelines and architectural principles

### **ğŸ¯ Development Guidelines**
- **Benchmark-Driven Development**: All features validated through comprehensive benchmarks
- **Real-World Testing**: Mainnet fork validation with actual deployed programs
- **Comprehensive Logging**: Detailed tracing for debugging and performance analysis
- **Modular Architecture**: Clean separation of concerns for maintainability

---

## ğŸ‰ Conclusion

The `reev` framework has successfully evolved from a proof-of-concept to a **production-ready evaluation platform** for Solana LLM agents. With comprehensive Jupiter integration, advanced multi-step workflows, and robust infrastructure, it now serves as the definitive tool for assessing autonomous agent capabilities in realistic blockchain environments.

The upcoming Phase 16 surfpool management improvements will further enhance the developer experience, making the framework even more accessible and efficient for both research and production use cases.